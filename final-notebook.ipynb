{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97919,"databundleVersionId":11694977,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**INSTALLING DEPENDENCIES**","metadata":{}},{"cell_type":"code","source":"# General packages\n!pip install numpy pandas matplotlib scipy\n\n# Audio processing\n!pip install librosa\n\n# Whisper (for transcription)\n!pip install git+https://github.com/openai/whisper.git \n!pip install torch  # Required by Whisper\n\n# Hugging Face Transformers (for BERT)\n!pip install transformers\n\n# LanguageTool for grammar checking\n!pip install language-tool-python\n\n# Scikit-learn (for ML model)\n!pip install scikit-learn\n\n!pip install textstat\n!pip install umap-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CREATING OF TRANSCRIPTS**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport whisper\n\n# Datasets loaded\ntrain_df = pd.read_csv('/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv')\ntest_df = pd.read_csv('/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv')\n\n# Define audio directories\nTRAIN_AUDIO_DIR = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train'\nTEST_AUDIO_DIR = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test'\n\n# Define output transcription directories\nOUTPUT_TRAIN_TXT_DIR = '/kaggle/working/train_transcriptions'\nOUTPUT_TEST_TXT_DIR = '/kaggle/working/test_transcriptions'\nos.makedirs(OUTPUT_TRAIN_TXT_DIR, exist_ok=True)\nos.makedirs(OUTPUT_TEST_TXT_DIR, exist_ok=True)\n\n# Load Whisper model\nmodel = whisper.load_model(\"large\")\n\n# Transcribe training audio files\nprint(\"Transcribing training audio files...\")\nfor filename in train_df['filename'].tolist():\n    full_path = os.path.join(TRAIN_AUDIO_DIR, filename)\n    audio = whisper.load_audio(full_path)\n    result = model.transcribe(audio)\n    transcription = result['text'].strip()\n    \n    txt_filename = os.path.splitext(filename)[0] + \".txt\"\n    with open(os.path.join(OUTPUT_TRAIN_TXT_DIR, txt_filename), 'w') as f:\n        f.write(transcription)\n\n# Transcribe test audio files\nprint(\"Transcribing test audio files...\")\nfor filename in test_df['filename'].tolist():\n    full_path = os.path.join(TEST_AUDIO_DIR, filename)\n    audio = whisper.load_audio(full_path)\n    result = model.transcribe(audio)\n    transcription = result['text'].strip()\n    \n    txt_filename = os.path.splitext(filename)[0] + \".txt\"\n    with open(os.path.join(OUTPUT_TEST_TXT_DIR, txt_filename), 'w') as f:\n        f.write(transcription)\n\nprint(\"âœ… All transcriptions saved:\")\nprint(f\" - Train: {OUTPUT_TRAIN_TXT_DIR}\")\nprint(f\" - Test:  {OUTPUT_TEST_TXT_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport matplotlib.pyplot as plt\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom lightgbm import LGBMRegressor\nimport optuna\nfrom language_tool_python import LanguageTool\nfrom tqdm import tqdm\n\n# Paths\nTRAIN_CSV = '/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv'\nTEST_CSV = '/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv'\nTRAIN_TRANSCRIPT_DIR = '/kaggle/input/shl-dataset/train_transcriptions'\nTEST_TRANSCRIPT_DIR = '/kaggle/input/shl-dataset/test_transcriptions'\nTRAIN_AUDIO_DIR = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train'\nTEST_AUDIO_DIR = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test'\n\n# Load data\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\n\n# Models\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nroberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nroberta_model = RobertaModel.from_pretrained(\"roberta-base\").to(device).eval()\nsentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\ntool = LanguageTool(\"en-US\")\n\ndef get_text(path):\n    with open(path, 'r') as f:\n        return f.read().strip()\n\ndef get_roberta_embedding(text):\n    inputs = roberta_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n    with torch.no_grad():\n        outputs = roberta_model(**inputs)\n    return outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n\ndef get_sentence_embedding(text):\n    return sentence_model.encode(text)\n\ndef get_grammar_features(text):\n    matches = tool.check(text)\n    grammar = sum(1 for m in matches if 'GRAMMAR' in m.category)\n    typo = sum(1 for m in matches if 'TYPO' in m.category)\n    style = sum(1 for m in matches if 'STYLE' in m.category)\n    return [grammar, typo, style]\n\ndef get_audio_features(audio_path, text):\n    y, sr = librosa.load(audio_path)\n    duration = librosa.get_duration(y=y, sr=sr)\n    word_count = len(text.split())\n    speaking_rate = word_count / (duration / 60) if duration else 0\n    pauses = len(librosa.effects.split(y)) - 1\n    return [speaking_rate, max(pauses, 0)]\n\ndef extract_features(df, transcript_dir, audio_dir, tfidf_model=None, is_train=True):\n    features = []\n    tfidf_texts = []\n    labels = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        fname = row['filename']\n        text_path = os.path.join(transcript_dir, fname.replace('.wav', '.txt'))\n        audio_path = os.path.join(audio_dir, fname)\n        \n        text = get_text(text_path)\n        tfidf_texts.append(text)\n\n        roberta_emb = get_roberta_embedding(text)\n        sentence_emb = get_sentence_embedding(text)\n        grammar_feats = get_grammar_features(text)\n        audio_feats = get_audio_features(audio_path, text)\n        sentence_len = np.mean([len(s.split()) for s in text.split('.') if s.strip()]) or 0\n\n        feat = np.concatenate([roberta_emb, sentence_emb, grammar_feats, audio_feats, [sentence_len]])\n        features.append(feat)\n\n        if is_train:\n            labels.append(row['label'])\n\n    if is_train and tfidf_model is None:\n        tfidf_model = TfidfVectorizer(max_features=300)\n        tfidf_model.fit(tfidf_texts)\n    \n    tfidf_feats = tfidf_model.transform(tfidf_texts).toarray()\n    all_feats = np.hstack([features, tfidf_feats])\n\n    return np.nan_to_num(all_feats), (np.array(labels) if is_train else None), tfidf_model\n\n# Extract Features\nX_train, y_train, tfidf_model = extract_features(train_df, TRAIN_TRANSCRIPT_DIR, TRAIN_AUDIO_DIR)\nX_test, _, _ = extract_features(test_df, TEST_TRANSCRIPT_DIR, TEST_AUDIO_DIR, tfidf_model, is_train=False)\n\n# Optuna Objective\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n    }\n    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n    model = LGBMRegressor(**params)\n    model.fit(X_tr, y_tr)\n    preds = model.predict(X_val)\n    return mean_squared_error(y_val, preds)\n\n# Run Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=30)\n\nprint(\"Best params:\", study.best_params)\n\n# Train final model\nfinal_model = LGBMRegressor(**study.best_params)\nfinal_model.fit(X_train, y_train)\n\n# Predict and Save\npreds = final_model.predict(X_test)\npreds = np.clip(preds, 0, 5)\n\nsubmission = pd.DataFrame({'filename': test_df['filename'], 'label': preds})\nsubmission.to_csv('testing.csv', index=False)\nprint(\"Final submission saved as 'submission.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}